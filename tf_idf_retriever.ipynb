{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from typing import List,Dict\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\omer.l\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omer.l\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "from string import punctuation, ascii_lowercase\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "N_ROWS_FOR_DEBUG = 5*10**3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data\n",
    "`Download the data` from [HERE](https://github.com/ludovicaschaerf/TMCI_Project/raw/master/data/380000-lyrics-from-metrolyrics.zip) and put it in the directory the script is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_PATH = Path(\"lyrics.csv\")\n",
    "BOW_PATH = Path(\"bow.csv\")\n",
    "N_ROWS = N_ROWS_FOR_DEBUG if DEBUG else None\n",
    "CHUNCK_SIZE = 5 if DEBUG else 5*10**3\n",
    "tqdm_n_iterations = N_ROWS//CHUNCK_SIZE +1 if DEBUG else 363*10**3//CHUNCK_SIZE + 1\n",
    "COLS = [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Bag of words /TfIdf model\n",
    "### Implement the following methods:\n",
    "\n",
    "* `preprocess_sentence`: \n",
    "    * Lower case the word\n",
    "    * Ignores it if it's in the stopwords list\n",
    "    * Removes characters which are not in the allowed symbols\n",
    "    * Stems it and appends it to the output sentence\n",
    "    * Discards words with length <= 1\n",
    "    \n",
    "    \n",
    "* `update_counts_and_probabilities`: \n",
    "\n",
    "    * Update self.unigram count (the amount of time each word is in the text)\n",
    "    * Update self.bigram count (two consecutive word occurances)\n",
    "    * Update self.trigram count (three consecutive word occurances)\n",
    "    * Update inverted index: a dictionary with words as keys and the values is a dictionary - {'DocID' : word_count}   \n",
    "    \n",
    "* `compute_word_document_frequency`:\n",
    "\n",
    "   * For each word count the number of docs it appears in. For example , for the word 'apple' -\n",
    "$$\\sum_{i \\in docs} I(apple \\in doc_i), I := Indicator function$$\n",
    "\n",
    "\n",
    "* `update_inverted_index_with_tf_idf_and_compute_document_norm`:\n",
    "\n",
    "    * Update the inverted index (which currently hold word counts) with tf idf weighing. We will compute tf by dividing with the number of words in each document. \n",
    "    * As we want to calculate the document norm, incrementally update the document norm. pay attention that later we apply sqrt to it to finish the process.\n",
    "\n",
    "#### The result of this code is a bag of words model that already counts for TF-IDF weighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "allowed_symbols = set(l for l in ascii_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 73/73 [17:48<00:00, 14.64s/it]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence : str) -> List[str]:\n",
    "    output_sentence = []\n",
    "    for word in word_tokenize(sentence):\n",
    "        word = word.lower()\n",
    "        word = ''.join([i for i in word if i in allowed_symbols])\n",
    "        if(word in stop_words):\n",
    "            continue  \n",
    "        word = stemmer.stem(word)\n",
    "        if(len(word)>1):\n",
    "            output_sentence.append(word)\n",
    "    return output_sentence\n",
    "    \n",
    "\n",
    "def get_data_chuncks() -> List[str]:\n",
    "    for i ,chunck in enumerate(pd.read_csv(INPUT_FILE_PATH, usecols = COLS, chunksize = CHUNCK_SIZE, nrows = N_ROWS)):\n",
    "        chunck = chunck.values.tolist()\n",
    "        yield [chunck[i][0] for i in range(len(chunck))] \n",
    "\n",
    "class TfIdf:\n",
    "    def __init__(self):\n",
    "        self.unigram_count =  Counter()\n",
    "        self.bigram_count = Counter()\n",
    "        self.trigram_count = Counter()\n",
    "        self.document_term_frequency = Counter()\n",
    "        self.word_document_frequency = {}\n",
    "        self.inverted_index = {}\n",
    "        self.doc_norms = {}\n",
    "        self.n_docs = -1\n",
    "        self.sentence_preprocesser = preprocess_sentence\n",
    "        self.bow_path = BOW_PATH\n",
    "\n",
    "    def update_counts_and_probabilities(self, sentence :List[str],document_id:int) -> None:\n",
    "        sentence_len = len(sentence)\n",
    "        self.document_term_frequency[document_id] = sentence_len\n",
    "        for i,word in enumerate(sentence):\n",
    "            self.unigram_count[word] += 1 \n",
    "            if(i < len(sentence)-1):\n",
    "                self.bigram_count[(sentence[i],sentence[i+1])] += 1\n",
    "            if(i < len(sentence)-2):\n",
    "                self.trigram_count[(sentence[i],sentence[i+1],sentence[i+2])] += 1\n",
    "            if word not in self.inverted_index:\n",
    "                self.inverted_index.update({word:{document_id:1}})\n",
    "            else:\n",
    "                if(document_id in self.inverted_index[word]):\n",
    "                    self.inverted_index[word][document_id] += 1\n",
    "                else:\n",
    "                    self.inverted_index[word].update({document_id: 1}) \n",
    "                    \n",
    "    def fit(self) -> None:\n",
    "        for chunck in tqdm(get_data_chuncks(), total = tqdm_n_iterations):\n",
    "            for sentence in chunck:\n",
    "                self.n_docs += 1 \n",
    "                if not isinstance(sentence, str):\n",
    "                    continue\n",
    "                sentence = self.sentence_preprocesser(sentence)\n",
    "                if sentence:\n",
    "                    self.update_counts_and_probabilities(sentence,self.n_docs)\n",
    "        self.save_bow() # bow is 'bag of words'\n",
    "        self.compute_word_document_frequency()\n",
    "        self.update_inverted_index_with_tf_idf_and_compute_document_norm()\n",
    "             \n",
    "    def compute_word_document_frequency(self):\n",
    "        for word in self.inverted_index.keys():\n",
    "            self.word_document_frequency[word] = len(self.inverted_index[word])\n",
    "            \n",
    "    def update_inverted_index_with_tf_idf_and_compute_document_norm(self):\n",
    "        for term in self.inverted_index:\n",
    "            for doc, freq in self.inverted_index[term].items():\n",
    "                self.inverted_index[term][doc] = (freq / self.document_term_frequency[doc] * np.log10(self.n_docs/self.word_document_frequency[term]))\n",
    "                if doc not in self.doc_norms:\n",
    "                    self.doc_norms.update({doc:0}) \n",
    "                self.doc_norms[doc] += (self.inverted_index[term][doc]**2)\n",
    "        for doc in self.doc_norms.keys():\n",
    "            self.doc_norms[doc] = np.sqrt(self.doc_norms[doc]) \n",
    "            \n",
    "    def save_bow(self):\n",
    "        pd.DataFrame([self.inverted_index]).T.to_csv(self.bow_path)\n",
    "                \n",
    "tf_idf = TfIdf()\n",
    "tf_idf.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Bag of words model:\n",
    "\n",
    "1. What is the computational complexity of this model, as a function of the number of docs in the corpus?\n",
    "2. How can we make this code better in terms running time (parallelization or other topics you find)? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. \n",
    "Let the number of docs be- d, and the number of average words in a document be v and we assume v<<d.\n",
    "On each document (d) we do v processing operations, another v operations for unigram count and, another v for inverted index, and another v operation for updating the inverted index. therefore, even when including arithmetic operations, we still meet complexity of O(v).  \n",
    "we have d documents. thus, we get a complexity of O(d*v). \n",
    "\n",
    "(and in the worst case if v=d -> O(d^2) will be an upper bound)\n",
    "\n",
    "2.\n",
    "we can use parallel processing - almost all the computers these days come with processors with multiple cores. we can seperate the job to multiple threads that are independent on each other. each will deal a different chunck of the data. now we have multiple chuncks of the data that are being computed at the same time on different cores. each of those cores will return us a dictionary of words (indexes) and counts. \n",
    "when the job is done we will merge the different dictionaries and then we will create the weights- this way we save time on the thing that takes us most of the time, that is running over each word of each document and create the counters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 DocumentRetriever\n",
    "Not this retriever &#8595;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![dsafdsafsdafdsf](https://cdn3-www.dogtime.com/assets/uploads/2019/10/golden-cocker-retriever-mixed-dog-breed-pictures-cover-1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentRetriever:\n",
    "    def __init__(self, tf_idf):\n",
    "        self.sentence_preprocesser = preprocess_sentence\n",
    "        self.vocab = set(tf_idf.unigram_count.keys())\n",
    "        self.n_docs = tf_idf.n_docs\n",
    "        self.inverted_index = tf_idf.inverted_index\n",
    "        self.word_document_frequency = tf_idf.word_document_frequency\n",
    "        self.doc_norms = tf_idf.doc_norms\n",
    "\n",
    "    def rank(self, query: Dict[str, int], documents: Dict[str, Counter], metric: str) -> Dict[str, float]:\n",
    "        result = {}  # key: DocID , value : float , simmilarity to query\n",
    "        query_len = np.sum(np.array(list(query.values())))\n",
    "        for term, count in query.items(): #in this loop we're updating the query's weights \n",
    "            query[term] = (count / query_len * np.log10(tf_idf.n_docs / tf_idf.word_document_frequency[term]))\n",
    "            for doc, freq in documents[term].items():\n",
    "                if (doc not in result):\n",
    "                    result.update({doc: 0})\n",
    "                if metric == 'inner_product':\n",
    "                    result[doc] += query[term] * freq\n",
    "                if metric == 'cosine':\n",
    "                    result[doc] += (query[term] * freq / self.doc_norms[doc])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def sort_and_retrieve_k_best(self, scores: Dict[str, float], k: int):\n",
    "        ### YOUR CODE HERE\n",
    "        return list({k: v for k, v in sorted(scores.items(), key=lambda item: item[1],reverse=True)})[:k]\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def reduce_query_to_counts(self, query: List) :\n",
    "        return dict(Counter(query)) # rank get Dict as input so we used this cast (even that a counter is a dict)\n",
    "\n",
    "    def get_top_k_documents(self, query: str, metric: str, k=5) -> List[str]:\n",
    "        query = self.sentence_preprocesser(query)\n",
    "        query = [word for word in query if word in self.vocab]  # filter nan\n",
    "        query_bow = self.reduce_query_to_counts(query)\n",
    "        relavant_documents = {word: self.inverted_index.get(word) for word in query}\n",
    "        ducuments_with_similarity = self.rank(query_bow, relavant_documents, metric)\n",
    "        return self.sort_and_retrieve_k_best(ducuments_with_similarity, k)\n",
    "        \n",
    "dr = DocumentRetriever(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\omer.l\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\display.py:689: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KT6ZtUbVw1M?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "query = \"Better stop dreaming of the quiet life, 'cause it's the one we'll never know And quit running for that runaway bus 'cause those rosy days are few And stop apologizing for the things you've never done 'Cause time is short and life is cruel but it's up to us to change This town called malice\"\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KT6ZtUbVw1M?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66192, 98277, 318295, 234637, 107511]\n",
      "[66192, 61393, 128160, 309297, 318295]\n"
     ]
    }
   ],
   "source": [
    "cosine_top_k = dr.get_top_k_documents(query, 'cosine')\n",
    "print(cosine_top_k)\n",
    "inner_product_top_k = dr.get_top_k_documents(query, 'inner_product')\n",
    "print(inner_product_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "song #0 \n",
      "I hear people talk\n",
      "I see people walk\n",
      "They seem so out of touch\n",
      "I wanna get away so much\n",
      "All the clockwork toys\n",
      "Are making too much noise\n",
      "It's the machinery\n",
      "It's breaking down, oh can't you see\n",
      "Run runaway, run runaway\n",
      "Run runaway with me\n",
      "Run runaway, run runaway\n",
      "Run runaway with me\n",
      "The picture doesn't change\n",
      "It's just a frozen frame\n",
      "I wanna break the ice\n",
      "I wanna go to paradise\n",
      "There is nowhere to hide\n",
      "I'll take you for a ride\n",
      "But not if you kiss and tell\n",
      "I don't mean on a carousel\n",
      "Won't you run runaway, run runaway\n",
      "Run runaway with me\n",
      "Run runaway, run runaway\n",
      "Run runaway with me\n",
      "One day I'm to say my\n",
      "Three wishes came true\n",
      "Till then I pretend I'm\n",
      "Escaping with you, you, you, you\n",
      "Run runaway, run runaway\n",
      "Run runaway with me\n",
      "Run runaway, run runaway\n",
      "Run runaway with me\n",
      "Run runaway, run runaway\n",
      "Run runaway with me\n",
      "Run runaway, run runaway\n",
      "Run runaway with me\n",
      "Run runaway, run runaway\n",
      "Run runaway with me\n",
      "Run runaway, run runaway\n",
      "Run runaway with me\n",
      "Run runaway, run runaway\n",
      "Run runaway with me\n",
      "Run runaway, run runaway\n",
      "Run runaway with me \n",
      "##################################################\n",
      "##################################################\n",
      "song #1 \n",
      "Say it's true, there's nothing like me and you\n",
      "Not alone, tell me you feel it too\n",
      "And I would runaway\n",
      "I would runaway, yeah\n",
      "I would runaway\n",
      "I would runaway with you\n",
      "'Cause I have fallen in love\n",
      "With you, no never have\n",
      "I'm never gonna stop falling in love, with you\n",
      "Close the door, lay down upon the floor\n",
      "And by candlelight, make love to me through the night\n",
      "'Cause I have runaway\n",
      "I have runaway, yeah\n",
      "I have runaway, runaway\n",
      "I have runaway with you\n",
      "'Cause I have fallen in love\n",
      "With you, no never have\n",
      "I'm never gonna stop falling in love, with you (with you)\n",
      "And I would runaway\n",
      "I would runaway, yeah\n",
      "I would runaway\n",
      "I would runaway with you\n",
      "'Cause I have fallen in love\n",
      "With you, no never have\n",
      "I'm never gonna stop falling in love, with you\n",
      "Fallen in love with you\n",
      "No never, ever, I'm never gonna stop falling in love, with you\n",
      "With you, my love\n",
      "With you\n",
      "Ya da da da da da da da da da da da \n",
      "##################################################\n",
      "##################################################\n",
      "song #2 \n",
      "She said she sad\n",
      "she sad she lonely\n",
      "lost in her room\n",
      "she mad she go\n",
      "she cry she smoke\n",
      "rosie in bloom\n",
      "she is so high\n",
      "she is so low\n",
      "day after day\n",
      "so\n",
      "she said she sad\n",
      "she sad she lonely\n",
      "open your eyes\n",
      "riding to rosie\n",
      "rosie sixteen\n",
      "rosie awakes in the morning\n",
      "riding to rosie\n",
      "where have you been\n",
      "rosie is sick of the weather\n",
      "long time no see\n",
      "long time no rosie\n",
      "I like to swim\n",
      "every now and then\n",
      "she swim she wet\n",
      "she out she dry\n",
      "play in the sand\n",
      "with a plastic boy\n",
      "riding to rosie\n",
      "rosie sixteen\n",
      "rosie is sick of the weather\n",
      "rinding to rosie\n",
      "where have you been\n",
      "rosie is never together\n",
      "she said she sad\n",
      "she said she lonely\n",
      "lost in her room\n",
      "she mad she go\n",
      "she cry she smoke\n",
      "rosie in bloom\n",
      "riding to rosie\n",
      "rosie sixteen\n",
      "rosie is sick of the weather\n",
      "riding to rosie\n",
      "where have you been\n",
      "rosie is never together \n",
      "##################################################\n",
      "##################################################\n",
      "song #3 \n",
      "Rosie you're my lucky charm\n",
      "When you're in my arms\n",
      "I'm happy as can be\n",
      "Oh Rosie, everytime we meet\n",
      "My heart skips a beat\n",
      "I'm happy to repeat\n",
      "Oh Rosie Oh Rosie\n",
      "You are the one\n",
      "Tell me oh tell me\n",
      "You'll be my number one\n",
      "Hot doggie\n",
      "I tell ya\n",
      "Rosie give me your answer do\n",
      "I promise to be true\n",
      "Oh just you wait and see\n",
      "Oh Rosie Oh Rosie\n",
      "Don't make me lose my self control\n",
      "Girl\n",
      "Rosie Rosie you've got a little heart of gold\n",
      "Everybody loves ya\n",
      "I know the reason why\n",
      "You put the sunshine\n",
      "Way up in the sky\n",
      "There's really somethin'\n",
      "The way you drive me wild\n",
      "You make my heart sing\n",
      "With your Mona-Lisa smile\n",
      "Rosie Oh Rosie\n",
      "You are the one\n",
      "Tell me girl tell me\n",
      "You'll be my number one\n",
      "uh huh\n",
      "I tell ya\n",
      "Rosie give me your answer do\n",
      "I promise to be true\n",
      "Oh just you wait and see\n",
      "Oh Rosie Oh Rosie\n",
      "Don't make me lose my self control\n",
      "Girl\n",
      "I tell ya\n",
      "Rosie Rosie\n",
      "You've got a little heart of gold\n",
      "Rosie Rosie\n",
      "You've got a little heart of gold\n",
      "whistle...fade \n",
      "##################################################\n",
      "##################################################\n",
      "song #4 \n",
      "Rosie whatcha doing in this low class joint\n",
      "Dancing in the dark all day\n",
      "You used to be the darling of your high school scene\n",
      "Now you put your love on display\n",
      "Sweaty hands hand you up a dollar bill\n",
      "Hungry eyes never seem to get their fill\n",
      "I used to watch you walking down the hall\n",
      "Rosie do you see me when you hear them call your name\n",
      "Ohh Rosie, Rosie I wanna take you away\n",
      "Ohh Rosie, Rosie I'm gonna make you mine someday\n",
      "Rosie I went with you for that rose tattoo\n",
      "You promised no one else would see\n",
      "I used to wait and drive you home from dancing school\n",
      "Remember when you danced just for me\n",
      "Our love was deeper than the night was long\n",
      "But things just didn't work out like our favorite song\n",
      "I used to watch you walking down the hall\n",
      "Rosie do you see me when you hear them call your name\n",
      "Ohh Rosie, Rosie, I wanna take you away\n",
      "Ohh Rosie, Rosie, I'm gonna make you mine someday\n",
      "Ahh yeah ohh Rosie\n",
      "Ahh yeah ohh Rosie\n",
      "Do you remember our love was deeper than the night was long\n",
      "But things just didn't work out like our favorite song\n",
      "I used to watch you walking down the hall\n",
      "Rosie do you see me when you hear them call your name, Rosie\n",
      "Rosie, Rosie, I wanna take you away\n",
      "Ohh Rosie, Rosie, I'm gonna make you mine someday\n",
      "Ahh yeah ohh Rosie\n",
      "Ahh yeah ohh Rosie\n",
      "Ohh Rosie, Rosie I wanna take you away\n",
      "Ohh Rosie, Rosie, ohh Rosie\n",
      "Rosie can you hear me call you baby \n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "for index, song in enumerate(pd.read_csv(INPUT_FILE_PATH,usecols = [5]).iloc[cosine_top_k]['lyrics']):\n",
    "    sep = \"#\"*50\n",
    "    print(F\"{sep}\\nsong #{index} \\n{song} \\n{sep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 term statistics:\n",
    "Use \"bow\" object that we created earlier and answer the following questions:\n",
    "\n",
    "1. How many unique words we have?\n",
    "2. How many potential word bigrams we have? How many actual word bigrams we have? How do you explain this difference?\n",
    "3. What is the storage size of the input file \"lyrics.csv\"? What is the output file (bow.csv) size? how do you explain this difference?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388243\n",
      "6312785\n",
      "Original file (lyrics.csv) size is 324632382 bytes, and Bow file size is 193319838 bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n### Your verbal solution here\\n\\nOriginal file (lyrics.csv) size is 324632382 bytes, and Bow file size is 193319838 bytes- we managed to decrease the file size in about 40%\\nbecause we cleaned up repetitive words and many other words that we omitted in the sentece processing phase. we ended up with smaller and \\n(hopefully) more informative data.\\n\\n### End your verbal solution here\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. \n",
    "df = pd.read_csv(BOW_PATH)\n",
    "print(df.shape[0])\n",
    "\n",
    "\"\"\"\n",
    "### Your verbal solution here\n",
    "we have 388243 unique words!\n",
    "### End your verbal solution here\n",
    "\"\"\"\n",
    "\n",
    "# 2.\n",
    "print(len(tf_idf.bigram_count))\n",
    "\n",
    "\"\"\"\n",
    "### Your verbal solution here\n",
    "we could potentialy have n^2 words but we took only the consecutive (and not all the possible pair of the words). this is why we ended up with \n",
    "a number than is not enormous.\n",
    "\n",
    "### End your verbal solution here\n",
    "\"\"\"\n",
    "\n",
    "# 3.\n",
    "print('Original file (lyrics.csv) size is {} bytes, and Bow file size is {} bytes'.format(INPUT_FILE_PATH.stat().st_size,BOW_PATH.stat().st_size))\n",
    "\n",
    "\"\"\"\n",
    "### Your verbal solution here\n",
    "\n",
    "Original file (lyrics.csv) size is 324632382 bytes, and Bow file size is 193319838 bytes- we managed to decrease the file size in about 40%\n",
    "because we cleaned up repetitive words and many other words that we omitted in the sentece processing phase. we ended up with smaller and \n",
    "(hopefully) more informative data.\n",
    "\n",
    "### End your verbal solution here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 NgramSpellingCorrector\n",
    "Now we will implement a Ngarm (character Ngrams) spelling corrector. That is, we have an out of vocabulary word (w) and we want to retrieve the most similar words (in our vocabulary) to this word.\n",
    "we will model the similarity of two words by-\n",
    "\n",
    "$$sim(v,w) := prior \\cdot likelihood = p(w) \\cdot P(v|w) $$ \n",
    "$$P(v|w) := JaccardIndex =  \\frac{|X \\cap Y|}{|X \\cup Y|}$$\n",
    "\n",
    "Where v is an out of vocabulary word (typo or spelling mistake), w is in a vocabulary word, X is the ngram set of v and Y is the ngram set of w.\n",
    "For example, if n == 3, the set of ngrams for word \"banana\" is set(\"ban\",\"ana\",\"nan\",\"ana\") = {\"ban\",\"ana\",\"nan\"}\n",
    "\n",
    "In order to do it efficently, we will first construct an index from the possible Ngrams we have seen in our corpus to the words that those Ngrams appear in, in order prevent comparing w to all of the words in our corpus.\n",
    "Then, we will implement a function that computes this similarity.\n",
    "\n",
    "* Make sure you compute the JaccardIndex efficently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(word):\n",
    "    for ngram in nltk.ngrams(word, 2):\n",
    "        yield \"\".join(list(ngram))\n",
    "    \n",
    "def get_trigrams(word):\n",
    "    for ngram in nltk.ngrams(word, 3):\n",
    "        yield \"\".join(list(ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramSpellingCorrector:\n",
    "    def __init__(self, unigram_counts: Counter, get_n_gram: callable):\n",
    "        self.unigram_counts = unigram_counts\n",
    "        self.ngram_index = {}\n",
    "        self.get_n_grams = get_n_gram\n",
    "    \n",
    "    def build_index(self) -> None:\n",
    "        for word in self.unigram_counts:\n",
    "            for ngram in self.get_n_grams(word):\n",
    "                if ngram not in self.ngram_index:\n",
    "                    self.ngram_index[ngram] = set()\n",
    "                self.ngram_index[ngram].add(word)\n",
    "        \n",
    "    def get_top_k_words(self,word:str,k=5) -> List[str]:\n",
    "        relevant_ngrams = []\n",
    "        for ngram in self.get_n_grams(word):\n",
    "            relevant_ngrams.append(ngram)\n",
    "        list_of_words = []\n",
    "        for words in [self.ngram_index[x] for x in set(self.ngram_index.keys()).intersection(set(relevant_ngrams))]:\n",
    "            for word in words:\n",
    "                list_of_words.append(word)\n",
    "        intersection_counter = dict(Counter(list_of_words)) # we're getting a counter of each word and the # of shared ngrams it has with the given word\n",
    "        sum_unigram_values = sum(self.unigram_counts.values()) # compute it here once in order to compute p(w)\n",
    "        for key_word, count in intersection_counter.items():\n",
    "            intersection_counter[key_word] = (self.unigram_counts[key_word] / sum_unigram_values ) * (count / (len(word +key_word) - count))\n",
    "        return list({k: v for k, v in sorted(intersection_counter.items(), key=lambda item: item[1],reverse=True)})[:k]\n",
    "\n",
    "\n",
    "class BigramSpellingCorrector(NgramSpellingCorrector):\n",
    "    def __init__(self, unigram_counts: Counter):\n",
    "        super().__init__(unigram_counts, get_bigrams)\n",
    "        \n",
    "        \n",
    "class TrigramSpellingCorrector(NgramSpellingCorrector):\n",
    "    def __init__(self, unigram_counts: Counter):\n",
    "        super().__init__(unigram_counts, get_trigrams)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caus', 'like', 'life', 'still', 'time']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_of_vocab_word = 'supercalifragilisticexpialidocious'\n",
    "bigram_spelling_corrector = BigramSpellingCorrector(tf_idf.unigram_count)\n",
    "bigram_spelling_corrector.build_index()\n",
    "bigram_spelling_corrector.get_top_k_words(out_of_vocab_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['life', 'still', 'call', 'listen', 'hous']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_spelling_corrector = TrigramSpellingCorrector(tf_idf.unigram_count)\n",
    "trigram_spelling_corrector.build_index()\n",
    "trigram_spelling_corrector.get_top_k_words(out_of_vocab_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Language model\n",
    "Calculate the log likelihood of a sentence. Once with a bigram markovian langauge model, and once with a trigram model.\n",
    "for example - the likelihood of the senetence \"spiderman spiderman does whatever a spider can\" for the bigram model is: \n",
    "$$p(spiderman)\\cdot p(spiderman|spiderman) \\cdot  (does|spiderman) \\cdot (whatever|does) \\cdot  (a|whatever) \\cdot  (spider|a) \\cdot (can|spider)$$\n",
    "\n",
    "And for the trigram model:\n",
    "$$p(spiderman,spiderman)\\cdot p(does|spiderman,spiderman) \\cdot  (whatever|spiderman,does) \\cdot (a|does,whatever) \\cdot  (spider|whatever,a) \\cdot  (can|a, spider)$$\n",
    "\n",
    "Since we do not want a zero probability sentence use Laplace smoothing, as you have seen in the lecture, or here https://en.wikipedia.org/wiki/Additive_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram log likelihood is -80.90642826061624\n",
      "Trigram log likelihood is -67.22506352284744\n"
     ]
    }
   ],
   "source": [
    "# for the probability smoothing\n",
    "NUMERATOR_SMOOTHING = 1\n",
    "DENOMINATOR_SMOOTHING = 10**4\n",
    "def sentence_log_probabilty(unigrams : Counter, bigrams  : Counter,trigrams : Counter, sentence: str) :\n",
    "    bigram_log_likelilhood, trigram_log_likelilhood = 0, 0\n",
    "    splited_sentence = sentence.split()\n",
    "    for i,word in enumerate(splited_sentence):\n",
    "        bigram_d = sum([bigrams[x] for x in (y for y in bigrams.keys() if y[0] == splited_sentence[i-1])]) #denominator of p(word1|word2)- all word with contain i-1 word\n",
    "        trigram_d = sum([trigrams[x] for x in (y for y in trigrams.keys() if y[0:2] == (splited_sentence[i-2] ,splited_sentence[i-1]))]) # same for trigram \n",
    "        if(i==0):\n",
    "            bigram_log_likelilhood += np.log((unigrams[word] +NUMERATOR_SMOOTHING) / (sum(unigrams.values()) + DENOMINATOR_SMOOTHING))\n",
    "        if (i>0):\n",
    "            bigram_log_likelilhood += np.log((bigrams[(splited_sentence[i-1],splited_sentence[i])] + NUMERATOR_SMOOTHING) / (bigram_d + DENOMINATOR_SMOOTHING))\n",
    "        if i==1:\n",
    "            trigram_log_likelilhood += np.log((bigrams[(splited_sentence[i-1],splited_sentence[i])]+NUMERATOR_SMOOTHING) / (bigram_d +DENOMINATOR_SMOOTHING))\n",
    "        if (i>1):\n",
    "                trigram_log_likelilhood += np.log((trigrams[(splited_sentence[i-2],splited_sentence[i-1],splited_sentence[i])] +NUMERATOR_SMOOTHING) / (trigram_d+ DENOMINATOR_SMOOTHING))\n",
    "    print(F\"Bigram log likelihood is {bigram_log_likelilhood}\")\n",
    "    print(F\"Trigram log likelihood is {trigram_log_likelilhood}\")\n",
    "    \n",
    "sentence = \"spider man spider man does whatever a spider can\"\n",
    "sentence_log_probabilty(tf_idf.unigram_count, tf_idf.bigram_count, tf_idf.trigram_count, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.51 Language model: B\n",
    "For each model what is the next word prediciton for the sentnence \"i am\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The words in \"i am\" have been ommited in the sentence processing phase. therefore, we won't see the words 'i' or 'am' in our unigram and nor \n",
    "# in the bigram or trigram. what would make sense to do in this case is maybe to get the word with the highest likelihood in the corpus.\n",
    "# for the generic case of next word prediction given 2 other words we can compute the p(word_3|word_1,word_2) and take the word (word_3) with the highest probability\n",
    " \n",
    "filtered_dict = { key:value for (key,value) in tf_idf.trigram_count.items() if key[:2] == ('i','am')}\n",
    "filtered_dict \n",
    "#  this dict is empty as we said. if it wasn't, we'd take the key with the highst count (would imply that this word has the \n",
    "              #  highest probability to apear after 'i am')\n",
    "              #if we had to take a random choice, we would take the the word that appears the most in the corpus \n",
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
